# Safety-J: Evaluating Safety with Critique
This is the official repository for 

## ðŸ”¥News
- [2024/07/15] 

## Table of Contents

- [Introduction](#introduction)
- [How to use?](#how-to-use)
  - [Setup](#setup)
  - [Load Data](#load-data)
  - [Inference](#inference)
  - [Evaluation](#evaluation)
  - [Submit your result](#submit-your-result)
- [Citation](#citation)

## Introduction

<p align="center"> <img src="images/method.png" style="width: 80%;" id="title-icon">       </p>

**Safety-J** is an advanced bilingual (English and Chinese) safety evaluator designed to assess the safety of content generated by Large Language Models (LLMs). It provides detailed critiques and judgments, setting a new standard in AI content safety evaluation. It is featured with:

- **Bilingual Capability**: Evaluates content in both English and Chinese.
- **Critique-based Judgment**: Offers detailed critiques alongside safety classifications.
- **Iterative Preference Learning**: Continuously improves through an innovative iterative learning process.
- **Comprehensive Coverage**: Addresses a wide range of safety scenarios, from privacy concerns to ethical issues.
- **Meta-evaluation Framework**: Includes an automated benchmark for assessing critique quality.
- **State-of-the-art Performance**: Outperforms existing open-source models and strong proprietary models like GPT-4o in safety evaluation tasks.

## How to use?
### Setup
To begin using Safety-J, you need to install the required dependencies. You can do this by running the following command:
```bash
https://github.com/GAIR-NLP/Safety-J.git 
conda create -n Safety-J python=3.10  
conda activate Safety-J
cd Safety-J
pip install -r requirements.txt
```
